
\documentclass[11pt]{article}

\usepackage{common}
\usepackage{hyperref}
\title{HW1: Text Classification}
\author{Akhil Ketkar \\ akhilketkar@g.harvard.edu \\ \url{https://github.com/akhilketkar/CS287_HW1}}

\begin{document}
\maketitle{}

\section{Introduction}

The objective of this assignment is to classify text into categories. For
instance, we might be given a movie review and asked to predict the rating
that the movie was given by the reviewer. Other examples might be classifying
an email as spam or not spam. Alternatively classifying it as
important or not important as GMail now does. We use 3 linear models from
\citep{murphy2012machine} for this assignment.

\section{Problem Description}

\begin{itemize}
  \item \textbf{Input Representation}: We represent the input text using some set of features
  $\mcF$. Each data point is represented by a vector $\boldx$ of size
  $d_{in} = |\mcF|$ where $x_{f_i} = 1$ if that feature is "on" and 0 otherwise.
  In practice we use a sparse representation to store $\boldx$ since most of
  it's entries are 0.

  \item \textbf{Output Representation}: The output categories are elements of a set
  $\mcC$ where $d_{out} = |\mcC|$
  and are represented by one-hot vectors $\boldy \in \{0,1\}^{d_{out}}$ where
  $y_i = 1$ if the piece of text is in category $i \in \mcC$.

  \item \textbf{Learning Representation}:  So the goal is to learn a function
  $\bold{\hat{y}} = f(\boldx)$ that maps the
  feature based representation of the text to a score vector $\bold{\hat{y}}$. The
  highest scoring class is our prediction of the output.
  $\hat{c} = \argmax_{i \in \mcC} \hat{y}_i$

  \item \textbf{Evaluation}: We measure how well the algorithm performs by simply
  counting the percentage
  of time we get the right answer. $\sum_{i=1}^N \frac{\bold1(y_i = \hat{c})}{N}$

\end{itemize}

\section{Model and Algorithms}

For all the models below (unless mentioned otherwise) we use bag of words
features. i.e $d_{in}$ is the number of unique words in all the text samples.
Thus each piece of text is represented simply by turning "on" the feature
corresponding to each unique word in the text.

The models we use are also all linear. i.e The functions we will try to learn
will be of the form $\bold{\hat{y}} = f(\boldx \boldW + \boldb)$ where $\boldW,\boldb$
are the parameters. In each model we will define a loss function that will
capture some notion of doing badly at the given task (text classification in
this case). We will then perform a straightforward minimization of the loss
function. The "learning" will be the set of parameters that produce the minima.

\begin{itemize}
  \item \textbf{Multinomial Naive Bayes}
  In this model we assume that the features are conditionally independent of each
  other given $\boldy_i$ and the parameters. We also assume that the features are
  generated from a multinomial distribution. The loss function $L$ we use here is the
  negative joint log-likelihood.

  $$L(\boldW,\boldb) = - \sum_{i=1}^N \log p(\boldx_i,\boldy_i | \boldW,\boldb)$$

  Using the chain rule we can simplify the RHS into a marginal and a conditional.
  Independence further simplifies the expression giving

  $$L(\boldW,\boldb) = - \sum_{i=1}^N \log \prod_{j=1}^{d_{in}}
    p(x_{f_i} = 1| \boldy_i, \boldW, \boldb) p(\boldy)$$

  We can find best estimates for both the terms in the above expression in closed
  form by simply counting how often a feature appears in a particular category
  and by counting the number of times a category is seen in the dataset.

  \item \textbf{Multinomial Logistic Regression}

  In this model we take a discriminative approach and estimate $p(\boldy|\boldx)$
  directly by using the softmax function which takes in any real vector and turns
  it into probabilities (each value in (0,1) and the sum = 1).
  $$ p(\bold{\hat{y}}|\boldx) = softmax(\boldx \boldW + \boldb) $$

  Then we use a cross-entropy loss function to compute how badly our probabilities
  differed from the actual.
  $$ L(\boldW, \boldb) = \sum_{i=1}^{N} L_{cr}(\bold{\hat{y}_i},\boldy_i) $$
  where
  $$ L_{cr}(\bold{\hat{y}},\boldy) = - \sum_{j=1}^{d_{out}} y_j \log \hat{y}_j $$

  Given that $\boldy$ is a one-hot vector this simplifies to a single term. If
  we try to optimize this loss, the weights will blow up to infinity so we add a
  $l_2$ regularization term. There is no closed form solution to this optimization
  so we have to use some form of gradient decent. Here we've used mini-batch
  stochastic gradient decent.

  \item \textbf{Linear SVM}

  In this model we remove the probabalistic interpretation of
  $\bold{\hat{y}} = \boldx \boldW + \boldb$ and simply treat it as a score.

  We then use the hinge loss function to compute the loss of our score.
  $$ L(\boldW, \boldb) = \sum_{i=1}^{N} L_{h}(\bold{\hat{y}_i},\boldy_i) $$
  and

  $$ L_h(\bold{\hat{y}},\boldy) = \max \{0,1 - (\hat{y}_c - \hat{y}_{c'}) \}$$

  where $c$ is the true class i.e $y_c = 1$ and $c'$ is the class to which
  we assigned the highest score that was not the true class i.e
  $c' = argmax_{d \in \mcC \setminus \{c\}} \hat{y}_d$

\end{itemize}



\section{Experiments}

The baseline for all these models should be 0.2 since thats the score we would
get if we assigned labels at random.

Settings used for LR: th HW1.lua -datafile SST1.hdf5 -classifier lg -M 100
-N 10000 -eta 0.05 -lambda 0.8

\begin{table}[h]
\centering
\begin{tabular}{llr}
 \toprule
 Model &  & Acc. \\
 \midrule
 \textsc{Baseline 1} & & 0.20\\
 \textsc{Model 1: NB} & &  \\
 \textsc{Model 2: LG} & & 0.41719 \\
 \textsc{Model 3: SVM} & & 7.49\\
 \bottomrule
\end{tabular}
\caption{\label{tab:results} Table with the main results.}
\end{table}

\section{Conclusion}

Some progress has been made over random assignment, but accuracy is still quite
low.

Potential improvements:

\begin{itemize}
  \item Better Features
  We're ignoring the structure inherent in the data. Not taking into account
  qualifying terms like "but", "not" etc.

  \item Better Model
\end{itemize}

\bibliographystyle{apalike}
\bibliography{writeup}

\end{document}
