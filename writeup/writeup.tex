
\documentclass[11pt]{article}

\usepackage{common}
\title{HW1: Text Classification}
\author{Akhil Ketkar \\ akhilketkar@g.harvard.edu}
\begin{document}

\maketitle{}
\section{Introduction}

The objective of this assignment is to classify text into categories. For
instance we might be given a movie review and asked to predict the rating
that the movie was given by the reviewer. But there are other examples such
as classfiying an email as spam or not spam. Alternatively classifying it as
important or not important as GMail now does.

We use 3 linear models from \citep{murphy2012machine} for this assignment.

\section{Problem Description}

More formally we want to learn a fucntion that maps text to a category. We
represent the text using some set of features $\mcF$. Each data point is
represented by a vector $\boldx$ of size $|\mcF|$ where $x_{f_i} = 1$ if
that feature is "on" and 0 otherwise.

The categories are elements of a set $\mcC$ and are represented by one-hot
vectors $\boldy \in \{0,1\}^{|\mcC|}$ where $y_i = 1$ if the piece of text is
in category $i$.

So the goal is to learn a function $\boldy = f(\boldx)$ that maps the feature
based representation of the text to the appropriate category.


\section{Model and Algorithms}

For all the models below (unless mentioned otherwise) we use bag of words
features. i.e The cardinality of $\mcF$ is the length of the vocabulary and
each piece of text is represented simply by turning "on" the feature
corresponding to each unique word in the text.

The models we use are also all linear. i.e The functions we will try to learn
will be of the form $\boldy = f(\boldx \boldW + \boldb)$ where $\boldW,\boldb$
are the parameters.

\begin{itemize}
  \item Naive Bayes
  \item Multinomial Logistic Regression
  \item Linear SVM
\end{itemize}

Here you specify the model itself. This section should formally
describe the model used to solve the task proposed in the previous
section. This section should try to avoid introducing new vocabulary
or notation, when possible use the notation from the previous section.
Feel free to use the notation from class, but try to make the note
understandable as a standalone piece of text.

This section is also a great place to include other material that
describes the underlying structure and choices of your model, for
instance here are some example tables and algorithms from full
research papers:



\begin{itemize}
\item diagrams of your model,

  \begin{center}
    \includegraphics[width=0.4\textwidth]{network}
  \end{center}
\item feature tables,

  \begin{center}
    \begin{tabular}{@{}lll@{}}
      \toprule
      &\multicolumn{2}{c}{Mention Features  } \\
      & Feature & Value Set\\
      \midrule
      & Mention Head & $\mcV$ \\
      & Mention First Word & $\mcV$ \\
      & Mention Last Word & $\mcV$ \\
      & Word Preceding Mention & $\mcV$ \\
      & Word Following Mention & $\mcV$\\
      & \# Words in Mention & $\{1, 2, \ldots \}$ \\
      & Mention Type & $\mathcal{T}$ \\
      \bottomrule
    \end{tabular}
  \end{center}

\item pseudo-code,

  \begin{algorithmic}[1]
    \Procedure{Linearize}{$x_1\ldots x_N$, $K$, $g$}
    \State{$B_0 \gets \langle (\langle \rangle, \{1, \ldots, N\}, 0, \boldh_0, \mathbf{0})  \rangle$}
    \For{$m = 0, \ldots, M-1$ }
    \For{$k = 1, \ldots, |B_m|$}
    \For{$i \in \mcR$}
    \State{$(y, \mcR, s, \boldh) \gets \mathrm{copy}(B_m^{(k)})$}
    \For{word $w$ in phrase $x_i$}
    \State{$y \gets y $ append $w$ }
    \State{$s \gets s + \log q(w, \boldh) $ }
    \State{$\boldh \gets \delta(w, \boldh)$}
    \EndFor{}
    \State{$B_{m+|w_i|} \gets B_{m+|w_i|} + (y, \mcR - i, s,   \boldh)$}
    \State{keep top-$K$ of $B_{m+|w_i|}$ by $f(x, y) + g(\mcR)$}
    \EndFor{}
    \EndFor{}
    \EndFor{}
    \State{\Return{$B_{M}^{(k)}$}}
    \EndProcedure{}
  \end{algorithmic}

\end{itemize}


\section{Experiments}

Finally we end with the experimental section. Each assignment will make clear the main experiments and baselines that you should run. For these experiments you should present a main results table. Here we give a sample Table~\ref{tab:results}. In addition to these results you should describe in words what the table shows and the relative performance of the models.

Besides the main results we will also ask you to present other results
comparing particular aspects of the models. For instance, for word
embedding experiments, we may ask you to show a chart of the projected
word vectors. This experiment will lead to something like
Figure~\ref{fig:clusters}. This should also be described within the
body of the text itself.


\begin{table}[h]
\centering
\begin{tabular}{llr}
 \toprule
 Model &  & Acc. \\
 \midrule
 \textsc{Baseline 1} & & 0.45\\
 \textsc{Baseline 2} & & 2.59 \\
 \textsc{Model 1} & & 10.59  \\
 \textsc{Model 2} & &13.42 \\
 \textsc{Model 3} & & 7.49\\
 \bottomrule
\end{tabular}
\caption{\label{tab:results} Table with the main results.}
\end{table}


\begin{figure}
  \centering
  \includegraphics[width=6cm]{cluster_viz}
  \caption{\label{fig:clusters} Sample qualitative chart.}
\end{figure}


\section{Conclusion}

End the write-up with a very short recap of the main experiments and the main results. Describe any challenges you may have faced, and what could have been improved in the model.

\bibliographystyle{apalike}
\bibliography{writeup}

\end{document}
